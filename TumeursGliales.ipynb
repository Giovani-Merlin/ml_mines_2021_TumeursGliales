{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a508092-0ed2-411d-a7f9-d32060ba24c9",
   "metadata": {},
   "source": [
    "# Mini-projet : Classification de tumeurs gliales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ef50ae-8bbf-4da4-ae72-8f38881318ea",
   "metadata": {},
   "source": [
    "Les _gliomes_ ou _tumeurs gliales_ sont des tumeurs du glie, le tissu de soutien neuronal du cerveau. Elles sont classifiées en 4 grades anatomo-pathologiques, dont dépend la prise en charge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9571f5-f28c-4604-8496-a5b422f62d7b",
   "metadata": {},
   "source": [
    "Dans ce jeu de données, chaque observation est un gliome, décrit par l'expression de 4 434 gènes. L'expression d'un gène est une mesure de la quantité d'ARN correspondant à ce gène qui est présente dans la cellule. Schématiquement, l'ADN est transcrit en ARN, lequel est lui-même traduit en une protéine. Les protéines assurent une multitude de fonctions du vivant, mais mesurer leur quantité est difficile ; d'où l'intérêt d'utiliser les quantités d'ARN, bien que la correspondance ne soit pas immédiate. \n",
    "\n",
    "Chaque gliome de notre jeu de données est étiquetée en fonction de son grade. \n",
    "\n",
    "Le but de ce projet est de construire un classifieur qui détermine, sur la base de l'expression de ces 4 434 gènes, le grade d'un gliome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1970b2b-d686-4531-b771-6a5fd2d47ad2",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "1. Comparez les performances d'au moins deux algorithmes d'apprentissage sur ce problème de classification.\n",
    "\n",
    "__Attention :__\n",
    "* au _data leakage_ (ne pas utiliser les données sur lesquelles on évalue les modèles pour les entraîner ou prétraiter les données)\n",
    "* à la taille du jeu de données\n",
    "* au nombre de classes\n",
    "* à choisir une mesure de performance appropriée (justifiez votre choix)\n",
    "2. Identifiez, quand cela est possible, les gènes les plus importants pour les modèles que vous avez entraînés. S'agit-il des mêmes gènes\n",
    "* entre deux modèles obtenus grâce à un algorithme d'apprentissage différents ?\n",
    "* entre deux modèles obtenus en utilisant le même algorithme d'apprentissage sur des sous-échantillons différents des données ?\n",
    "\n",
    "N'oubliez pas de commenter et interpréter vos résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6274ace-a84a-4830-9872-92dfd3e54ede",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9d5f7c1-54a5-4772-9b7d-8799ea4c6fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "data_matrix = scipy.io.loadmat(\"gliome.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dcb2008-d59a-4e35-a12d-ae7017f4c427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 4434)\n"
     ]
    }
   ],
   "source": [
    "X = data_matrix['X']\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30f163fb-bb27-421e-a221-69d84791f721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "y = data_matrix['Y'][:, 0] \n",
    "print(y.shape)\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D'abord, nous divisons les données en test et train.\n",
    "Même si nous n'avons pas beaucoup de données, nous devons faire la division..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métrique\n",
    "Nous utiliserions normalement la métrique \"précision\" car nos données sont bien stratifiées, c'est-à-dire que nous avons la même quantité de classes pour chaque classe y_i.\n",
    "Cependant, nous pouvons penser que la précision de la classe 4 est plus importante que la précision de la classe 1, car lorsque nous identifions la classe 4, nous avons besoin de faire une opération et nous ne voulons pas faire des opérations sans être nécessaire. D'autre part, le rappel de la classe 1 est également très important, car si le gliome est découvert à un stade précoce, il peut être traité et sa détérioration peut être évitée. Nous pourrions donc utiliser la métrique (Précision_4 + Rappel_1) / 2.\n",
    "* Obs, je n'ai pas de connaissances médicales mais les idées présentées ici peuvent être facilement modifiées en ayant un spécialiste médical dans l'équipe.\n",
    "# Choix du modèle\n",
    "Nous allons choisir entre la régression logistique (pour être le modèle le plus simple), le SVM (parce que nous n'avons pas beaucoup de labels et d'exemples), la forêt aléatoire (car p est élevé) et le réseau de neurones en couches (pour être le modèle le plus complexe). Nous pourrions également tester le boosting, mais nous avons déjà beaucoup d'hyperparamètres à tester.\n",
    "## Hyperparamètres\n",
    "Nous pouvons penser que le choix du modèle lui-même est un hyperparamètre, cependant, nous devons optimiser chaque modèle pour obtenir une comparaison équitable entre chaque modèle.\n",
    "Pour ce faire, nous allons utiliser la validation croisée stratifiée (c'est-à-dire avec le même nombre de classes pour chaque division) avec 5 divisions (ainsi, nous entraînons le modèle avec 32 exemples - 4*(0,8 * 50)/5 - et le testons dans les 8 exemples restants). Ainsi, notre ensemble de données de \"validation\" est constitué de fractions de l'ensemble de données de \"training\".\n",
    "### Itérations\n",
    "Ici, seule l'itération \"la plus fine\" des hyperparamètres sera montrée. Par exemple, pour C dans MLP nous commençons avec C à [0.1,1,10,100] et ensuite [0.001,.0.01,.01] parce que les meilleurs résultats pour MLP étaient avec C = 0.1 dans l'itération précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR-l2-newton-cg-0.001: 0.775000 (0.407224) - 0.866667 (0.124722)\n",
      "LR-l2-newton-cg-0.01: 0.800000 (0.411775) - 0.866667 (0.124722)\n",
      "LR-l2-newton-cg-0.1: 0.800000 (0.411775) - 0.866667 (0.124722)\n",
      "LR-l2-lbfgs-0.001: 0.775000 (0.407224) - 0.866667 (0.124722)\n",
      "LR-l2-lbfgs-0.01: 0.800000 (0.411775) - 0.866667 (0.124722)\n",
      "LR-l2-lbfgs-0.1: 0.800000 (0.411775) - 0.866667 (0.124722)\n",
      "SVM-linear-0.1: 0.800000 (0.411775) - 0.866667 (0.124722)\n",
      "SVM-linear-0.5: 0.800000 (0.411775) - 0.866667 (0.124722)\n",
      "SVM-linear-1: 0.800000 (0.411775) - 0.866667 (0.124722)\n",
      "SVM-linear-5: 0.800000 (0.411775) - 0.866667 (0.124722)\n",
      "SVM-linear-10: 0.800000 (0.411775) - 0.866667 (0.124722)\n",
      "SVM-rbf-0.1: 0.450000 (0.411775) - 0.606667 (0.117662)\n",
      "SVM-rbf-0.5: 0.600000 (0.223607) - 0.726667 (0.099219)\n",
      "SVM-rbf-1: 0.650000 (0.349964) - 0.708333 (0.134371)\n",
      "SVM-rbf-5: 0.775000 (0.407224) - 0.866667 (0.124722)\n",
      "SVM-rbf-10: 0.775000 (0.407224) - 0.866667 (0.124722)\n",
      "RF-50-2: 0.725000 (0.349964) - 0.808333 (0.128019)\n",
      "RF-50-8: 0.725000 (0.349964) - 0.841667 (0.106719)\n",
      "RF-50-20: 0.725000 (0.349964) - 0.841667 (0.106719)\n",
      "RF-100-2: 0.700000 (0.411775) - 0.750000 (0.102062)\n",
      "RF-100-8: 0.775000 (0.428616) - 0.825000 (0.113039)\n",
      "RF-100-20: 0.775000 (0.428616) - 0.825000 (0.113039)\n",
      "RF-200-2: 0.775000 (0.407224) - 0.841667 (0.106719)\n",
      "RF-200-8: 0.800000 (0.411775) - 0.841667 (0.106719)\n",
      "RF-200-20: 0.800000 (0.411775) - 0.841667 (0.106719)\n",
      "RF-500-2: 0.725000 (0.407224) - 0.808333 (0.128019)\n",
      "RF-500-8: 0.800000 (0.411775) - 0.833333 (0.105409)\n",
      "RF-500-20: 0.800000 (0.411775) - 0.833333 (0.105409)\n",
      "MLP-(300,)-logistic-0.001: 0.725000 (0.381803) - 0.758333 (0.227303)\n",
      "MLP-(300,)-logistic-0.01: 0.725000 (0.381803) - 0.758333 (0.227303)\n",
      "MLP-(300,)-logistic-0.1: 0.775000 (0.305845) - 0.858333 (0.081650)\n",
      "MLP-(500,)-logistic-0.001: 0.700000 (0.357037) - 0.675000 (0.214735)\n",
      "MLP-(500,)-logistic-0.01: 0.700000 (0.357037) - 0.675000 (0.214735)\n",
      "MLP-(500,)-logistic-0.1: 0.725000 (0.407224) - 0.691667 (0.216667)\n",
      "MLP-(1000,)-logistic-0.001: 0.625000 (0.420448) - 0.600000 (0.226078)\n",
      "MLP-(1000,)-logistic-0.01: 0.625000 (0.420448) - 0.600000 (0.226078)\n",
      "MLP-(1000,)-logistic-0.1: 0.675000 (0.466846) - 0.650000 (0.185592)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_val_predict, StratifiedKFold\n",
    "from math import sqrt\n",
    "\n",
    "models = []\n",
    "for penalty in [\"l2\"]:\n",
    "    for solver in [\"newton-cg\",\"lbfgs\"]:\n",
    "        for C in [0.001,0.01,0.1]:\n",
    "            if penalty == \"l1\" and solver in [\"newton-cg\",\"lbfgs\",\"sag\"]: # not supported\n",
    "                continue\n",
    "            else:\n",
    "                models.append(((f'LR-{penalty}-{solver}-{C}'),LogisticRegression(penalty=penalty,C=C,max_iter=1000,solver=solver, random_state=random_state,n_jobs=10)))\n",
    "\n",
    "for kernel in [\"linear\",\"rbf\"]:\n",
    "    for C in [0.1,0.5,1,5,10]:\n",
    "        models.append(((f'SVM-{kernel}-{C}'),SVC(kernel=kernel,C=C,max_iter=1000, random_state=random_state)))\n",
    "\n",
    "for n_estimators in [50,100,200,500]:\n",
    "    for max_depth in [2,8,20]:\n",
    "        models.append(((f'RF-{n_estimators}-{max_depth}'),RandomForestClassifier(n_estimators=n_estimators,max_depth=max_depth, random_state=random_state,n_jobs=10)))\n",
    "\n",
    "for hidden_layer_sizes in [(300,),(400,),(150,)]:\n",
    "    for activation in [\"logistic\"]:\n",
    "        for alpha in [0.1,0.5,1,4]:\n",
    "            models.append(((f'MLP-{hidden_layer_sizes}-{activation}-{alpha}'),MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,activation=activation,alpha=alpha, batch_size=3, random_state=random_state,early_stopping=True)))\n",
    "\n",
    "def eval_models(models,n_classes = 4):\n",
    "    from sklearn.base import clone\n",
    "    results = {}\n",
    "\n",
    "    for name, model in models:\n",
    "        # K-fold stratifié manuellement pour plus de contrôle et pour pouvoir faire toutes les métriques\n",
    "        accuracy_list = []\n",
    "        model_fit_list = [] # Pour chaque fold\n",
    "        class_results = defaultdict(lambda: defaultdict(list))\n",
    "        kf = StratifiedKFold(n_splits=5, random_state=random_state, shuffle=True).split(X_train, y_train)\n",
    "        for train_index, test_index in kf:\n",
    "\n",
    "            X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
    "            y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            predict = model.predict(X_test_fold)\n",
    "            # Fonction facile pour faire la précision et le rappel (et f1) de chaque classe.\n",
    "            class_report = classification_report(y_test_fold, predict,zero_division=0,output_dict=True)\n",
    "            for i in range(n_classes):\n",
    "                class_results[i+1]['precision'].append(class_report[str(i+1)]['precision'])\n",
    "                class_results[i+1]['recall'].append(class_report[str(i+1)]['recall'])\n",
    "                class_results[i+1]['f1-score'].append(class_report[str(i+1)]['f1-score'])\n",
    "                class_results[i+1]['support'].append(class_report[str(i+1)]['support'])\n",
    "            accuracy_list.append(class_report['accuracy'])\n",
    "            model_fit_list.append(clone(model))\n",
    "\n",
    "        custom_metric_array = (np.array(class_results[4]['precision'] )+ np.array(class_results[1]['recall']))/2\n",
    "        custom_metric = np.mean(custom_metric_array)\n",
    "        custom_metric_std = np.std(custom_metric_array)\n",
    "        for i in range(1,n_classes+1):\n",
    "            class_results[i]['precision'] = sum(class_results[i]['precision']) / len(class_results[i]['precision'])\n",
    "            class_results[i]['recall'] = sum(class_results[i]['recall']) / len(class_results[i]['recall'])\n",
    "            class_results[i]['f1-score'] = sum(class_results[i]['f1-score']) / len(class_results[i]['f1-score'])\n",
    "\n",
    "        total_accuracy = sum(accuracy_list) / len(accuracy_list)\n",
    "        std_accuracy = np.std(accuracy_list)\n",
    "\n",
    "        results[name] = {\"class_results\":class_results, \"accuracy\":total_accuracy, \"std_accuracy\":std_accuracy, \"custom_metric\":custom_metric,\"custom_metric_std\":custom_metric_std, \"models\":model_fit_list}\n",
    "        print('%s: %f (%f) - %f (%f)' % (name, total_accuracy, sqrt(std_accuracy),custom_metric,custom_metric_std))\n",
    "    return results\n",
    "    \n",
    "results = eval_models(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_results_dict = {}\n",
    "for name,result in results.items():\n",
    "    if result['custom_metric'] > 0.7:\n",
    "        filtered_results_dict[name] = {\"custom_metric\":result['custom_metric'],\"custom_metric_std\":result['custom_metric_std'],\"accuracy\":result['accuracy'],\"std_accuracy\":result['std_accuracy'],\"model\":result['models']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open('results.pkl', 'wb') as f:\n",
    "    pkl.dump(filtered_results_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.pkl', 'rb') as f:\n",
    "    filtered_results_dict = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_models = pd.DataFrame.from_dict(filtered_results_dict,orient='index')\n",
    "df_models\n",
    "df_models.sort_values(by=['custom_metric',\"custom_metric_std\"],ascending=[False,True],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Maintenant, on cherche les modéles avec la plus grand \"custom_metric\" et la plus petite \"custom_metric_std\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>custom_metric</th>\n",
       "      <th>custom_metric_std</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>std_accuracy</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR-l2-newton-cg-0.001</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.124722</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.165831</td>\n",
       "      <td>[LogisticRegression(C=0.001, max_iter=1000, n_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR-l2-newton-cg-0.01</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.124722</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.169558</td>\n",
       "      <td>[LogisticRegression(C=0.01, max_iter=1000, n_j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR-l2-newton-cg-0.1</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.124722</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.169558</td>\n",
       "      <td>[LogisticRegression(C=0.1, max_iter=1000, n_jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR-l2-lbfgs-0.001</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.124722</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.165831</td>\n",
       "      <td>[LogisticRegression(C=0.001, max_iter=1000, n_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR-l2-lbfgs-0.01</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.124722</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.169558</td>\n",
       "      <td>[LogisticRegression(C=0.01, max_iter=1000, n_j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR-l2-lbfgs-0.1</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.124722</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.169558</td>\n",
       "      <td>[LogisticRegression(C=0.1, max_iter=1000, n_jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM-linear-0.1</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.124722</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.169558</td>\n",
       "      <td>[SVC(C=0.1, kernel='linear', max_iter=1000, ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM-linear-0.5</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.124722</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.169558</td>\n",
       "      <td>[SVC(C=0.5, kernel='linear', max_iter=1000, ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM-linear-1</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.124722</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.169558</td>\n",
       "      <td>[SVC(C=1, kernel='linear', max_iter=1000, rand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM-linear-5</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.124722</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.169558</td>\n",
       "      <td>[SVC(C=5, kernel='linear', max_iter=1000, rand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM-linear-10</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.124722</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.169558</td>\n",
       "      <td>[SVC(C=10, kernel='linear', max_iter=1000, ran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM-rbf-5</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.124722</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.165831</td>\n",
       "      <td>[SVC(C=5, max_iter=1000, random_state=42), SVC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM-rbf-10</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.124722</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.165831</td>\n",
       "      <td>[SVC(C=10, max_iter=1000, random_state=42), SV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP-(300,)-logistic-0.1</th>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.081650</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.093541</td>\n",
       "      <td>[MLPClassifier(activation='logistic', alpha=0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF-50-8</th>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.106719</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.122474</td>\n",
       "      <td>[RandomForestClassifier(max_depth=8, n_estimat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF-50-20</th>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.106719</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.122474</td>\n",
       "      <td>[RandomForestClassifier(max_depth=20, n_estima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF-200-2</th>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.106719</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.165831</td>\n",
       "      <td>[RandomForestClassifier(max_depth=2, n_estimat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF-200-8</th>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.106719</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.169558</td>\n",
       "      <td>[RandomForestClassifier(max_depth=8, n_estimat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF-200-20</th>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.106719</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.169558</td>\n",
       "      <td>[RandomForestClassifier(max_depth=20, n_estima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF-500-8</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.105409</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.169558</td>\n",
       "      <td>[RandomForestClassifier(max_depth=8, n_estimat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF-500-20</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.105409</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.169558</td>\n",
       "      <td>[RandomForestClassifier(max_depth=20, n_estima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF-100-8</th>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.113039</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.183712</td>\n",
       "      <td>[RandomForestClassifier(max_depth=8, n_jobs=10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF-100-20</th>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.113039</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.183712</td>\n",
       "      <td>[RandomForestClassifier(max_depth=20, n_jobs=1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF-50-2</th>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.128019</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.122474</td>\n",
       "      <td>[RandomForestClassifier(max_depth=2, n_estimat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF-500-2</th>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.128019</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.165831</td>\n",
       "      <td>[RandomForestClassifier(max_depth=2, n_estimat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP-(300,)-logistic-0.001</th>\n",
       "      <td>0.758333</td>\n",
       "      <td>0.227303</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.145774</td>\n",
       "      <td>[MLPClassifier(activation='logistic', alpha=0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP-(300,)-logistic-0.01</th>\n",
       "      <td>0.758333</td>\n",
       "      <td>0.227303</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.145774</td>\n",
       "      <td>[MLPClassifier(activation='logistic', alpha=0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF-100-2</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.102062</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.169558</td>\n",
       "      <td>[RandomForestClassifier(max_depth=2, n_jobs=10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM-rbf-0.5</th>\n",
       "      <td>0.726667</td>\n",
       "      <td>0.099219</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>[SVC(C=0.5, max_iter=1000, random_state=42), S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM-rbf-1</th>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.134371</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.122474</td>\n",
       "      <td>[SVC(C=1, max_iter=1000, random_state=42), SVC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           custom_metric  custom_metric_std  accuracy  \\\n",
       "LR-l2-newton-cg-0.001           0.866667           0.124722     0.775   \n",
       "LR-l2-newton-cg-0.01            0.866667           0.124722     0.800   \n",
       "LR-l2-newton-cg-0.1             0.866667           0.124722     0.800   \n",
       "LR-l2-lbfgs-0.001               0.866667           0.124722     0.775   \n",
       "LR-l2-lbfgs-0.01                0.866667           0.124722     0.800   \n",
       "LR-l2-lbfgs-0.1                 0.866667           0.124722     0.800   \n",
       "SVM-linear-0.1                  0.866667           0.124722     0.800   \n",
       "SVM-linear-0.5                  0.866667           0.124722     0.800   \n",
       "SVM-linear-1                    0.866667           0.124722     0.800   \n",
       "SVM-linear-5                    0.866667           0.124722     0.800   \n",
       "SVM-linear-10                   0.866667           0.124722     0.800   \n",
       "SVM-rbf-5                       0.866667           0.124722     0.775   \n",
       "SVM-rbf-10                      0.866667           0.124722     0.775   \n",
       "MLP-(300,)-logistic-0.1         0.858333           0.081650     0.775   \n",
       "RF-50-8                         0.841667           0.106719     0.725   \n",
       "RF-50-20                        0.841667           0.106719     0.725   \n",
       "RF-200-2                        0.841667           0.106719     0.775   \n",
       "RF-200-8                        0.841667           0.106719     0.800   \n",
       "RF-200-20                       0.841667           0.106719     0.800   \n",
       "RF-500-8                        0.833333           0.105409     0.800   \n",
       "RF-500-20                       0.833333           0.105409     0.800   \n",
       "RF-100-8                        0.825000           0.113039     0.775   \n",
       "RF-100-20                       0.825000           0.113039     0.775   \n",
       "RF-50-2                         0.808333           0.128019     0.725   \n",
       "RF-500-2                        0.808333           0.128019     0.725   \n",
       "MLP-(300,)-logistic-0.001       0.758333           0.227303     0.725   \n",
       "MLP-(300,)-logistic-0.01        0.758333           0.227303     0.725   \n",
       "RF-100-2                        0.750000           0.102062     0.700   \n",
       "SVM-rbf-0.5                     0.726667           0.099219     0.600   \n",
       "SVM-rbf-1                       0.708333           0.134371     0.650   \n",
       "\n",
       "                           std_accuracy  \\\n",
       "LR-l2-newton-cg-0.001          0.165831   \n",
       "LR-l2-newton-cg-0.01           0.169558   \n",
       "LR-l2-newton-cg-0.1            0.169558   \n",
       "LR-l2-lbfgs-0.001              0.165831   \n",
       "LR-l2-lbfgs-0.01               0.169558   \n",
       "LR-l2-lbfgs-0.1                0.169558   \n",
       "SVM-linear-0.1                 0.169558   \n",
       "SVM-linear-0.5                 0.169558   \n",
       "SVM-linear-1                   0.169558   \n",
       "SVM-linear-5                   0.169558   \n",
       "SVM-linear-10                  0.169558   \n",
       "SVM-rbf-5                      0.165831   \n",
       "SVM-rbf-10                     0.165831   \n",
       "MLP-(300,)-logistic-0.1        0.093541   \n",
       "RF-50-8                        0.122474   \n",
       "RF-50-20                       0.122474   \n",
       "RF-200-2                       0.165831   \n",
       "RF-200-8                       0.169558   \n",
       "RF-200-20                      0.169558   \n",
       "RF-500-8                       0.169558   \n",
       "RF-500-20                      0.169558   \n",
       "RF-100-8                       0.183712   \n",
       "RF-100-20                      0.183712   \n",
       "RF-50-2                        0.122474   \n",
       "RF-500-2                       0.165831   \n",
       "MLP-(300,)-logistic-0.001      0.145774   \n",
       "MLP-(300,)-logistic-0.01       0.145774   \n",
       "RF-100-2                       0.169558   \n",
       "SVM-rbf-0.5                    0.050000   \n",
       "SVM-rbf-1                      0.122474   \n",
       "\n",
       "                                                                       model  \n",
       "LR-l2-newton-cg-0.001      [LogisticRegression(C=0.001, max_iter=1000, n_...  \n",
       "LR-l2-newton-cg-0.01       [LogisticRegression(C=0.01, max_iter=1000, n_j...  \n",
       "LR-l2-newton-cg-0.1        [LogisticRegression(C=0.1, max_iter=1000, n_jo...  \n",
       "LR-l2-lbfgs-0.001          [LogisticRegression(C=0.001, max_iter=1000, n_...  \n",
       "LR-l2-lbfgs-0.01           [LogisticRegression(C=0.01, max_iter=1000, n_j...  \n",
       "LR-l2-lbfgs-0.1            [LogisticRegression(C=0.1, max_iter=1000, n_jo...  \n",
       "SVM-linear-0.1             [SVC(C=0.1, kernel='linear', max_iter=1000, ra...  \n",
       "SVM-linear-0.5             [SVC(C=0.5, kernel='linear', max_iter=1000, ra...  \n",
       "SVM-linear-1               [SVC(C=1, kernel='linear', max_iter=1000, rand...  \n",
       "SVM-linear-5               [SVC(C=5, kernel='linear', max_iter=1000, rand...  \n",
       "SVM-linear-10              [SVC(C=10, kernel='linear', max_iter=1000, ran...  \n",
       "SVM-rbf-5                  [SVC(C=5, max_iter=1000, random_state=42), SVC...  \n",
       "SVM-rbf-10                 [SVC(C=10, max_iter=1000, random_state=42), SV...  \n",
       "MLP-(300,)-logistic-0.1    [MLPClassifier(activation='logistic', alpha=0....  \n",
       "RF-50-8                    [RandomForestClassifier(max_depth=8, n_estimat...  \n",
       "RF-50-20                   [RandomForestClassifier(max_depth=20, n_estima...  \n",
       "RF-200-2                   [RandomForestClassifier(max_depth=2, n_estimat...  \n",
       "RF-200-8                   [RandomForestClassifier(max_depth=8, n_estimat...  \n",
       "RF-200-20                  [RandomForestClassifier(max_depth=20, n_estima...  \n",
       "RF-500-8                   [RandomForestClassifier(max_depth=8, n_estimat...  \n",
       "RF-500-20                  [RandomForestClassifier(max_depth=20, n_estima...  \n",
       "RF-100-8                   [RandomForestClassifier(max_depth=8, n_jobs=10...  \n",
       "RF-100-20                  [RandomForestClassifier(max_depth=20, n_jobs=1...  \n",
       "RF-50-2                    [RandomForestClassifier(max_depth=2, n_estimat...  \n",
       "RF-500-2                   [RandomForestClassifier(max_depth=2, n_estimat...  \n",
       "MLP-(300,)-logistic-0.001  [MLPClassifier(activation='logistic', alpha=0....  \n",
       "MLP-(300,)-logistic-0.01   [MLPClassifier(activation='logistic', alpha=0....  \n",
       "RF-100-2                   [RandomForestClassifier(max_depth=2, n_jobs=10...  \n",
       "SVM-rbf-0.5                [SVC(C=0.5, max_iter=1000, random_state=42), S...  \n",
       "SVM-rbf-1                  [SVC(C=1, max_iter=1000, random_state=42), SVC...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_models[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifica a coef dos modelos escolhidos ali, só usar o \"models\" que já tem um fit diferente pra cada fold (pergunta 1 parte 2) e compara os coef. \n",
    "# Pra pergunta 1 parte 2 só pegar o modelo (qualquer um da lista de modelos) dar fit no train inteiro e comparar modelos diferentes (e a eficacia deles no test)\n",
    "\n",
    "SVM importance: https://stackoverflow.com/questions/41592661/determining-the-most-contributing-features-for-svm-classifier-in-sklearn\n",
    "random forest importance: https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
    "mlp: on peut pas\n",
    "logistic_regression: utiliser \"coef_\"\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2c5d815ec1fe03f9c56d61fe27b8ffe51143b3b44a5b00413fd120b13315193"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
